{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Assignment #5\n",
    "## Luke Schwenke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment Goals\n",
    "\n",
    "1. Identify what is this company name, by looking at the entity distributions across both tweets and news articles\n",
    "2. Identify what other companies are most frequently mentioned along with your primary company\n",
    "\n",
    "* Analyze what companies are most frequently mentioned within the same document (tweet and news article)\n",
    "* While analyzing news articles, extract separate entities from titles and texts\n",
    "\n",
    "3. Identify most frequent locations of events, by extracting appropriate named entities\n",
    "* Locations may include countries, states, cities, regions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CPUs: 8\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "num_processors = multiprocessing.cpu_count() \n",
    "print(f'Available CPUs: {num_processors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 7 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers=num_processors-1, use_memory_fs=False, progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample contains 10,012 news articles\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://kokomoperspective.com/obituaries/jon-w-horton/article_b6ba8e1e-cb9c-11eb-9868-fb11b88b9778.html</td>\n",
       "      <td>2021-06-13</td>\n",
       "      <td>en</td>\n",
       "      <td>Jon W. Horton | Obituaries | kokomoperspective.com</td>\n",
       "      <td>Jon W. Horton | Obituaries | kokomoperspective.comYou have permission to edit this article. EditCloseSign Up                        Log In                    Dashboard  LogoutMy Account Dashboard Profile Saved items LogoutCOVID-19Click here for the latest local news on COVID-19HomeAbout UsContact UsNewsLocalOpinionPoliticsNationalStateAgricultureLifestylesEngagements/Anniversaries/WeddingsAutosEntertainmentHealthHomesOutdoorsSportsNFLNCAAVitalsObituariesAutomotivee-EditionCouponsGalleries74¬∞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://auto.economictimes.indiatimes.com/news/auto-components/birla-precision-to-ramp-up-capacity-to-tap-emerging-opportunities-in-india/81254902</td>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>en</td>\n",
       "      <td>Birla Precision to ramp up capacity to tap emerging opportunities in India, Auto News, ET Auto</td>\n",
       "      <td>Birla Precision to ramp up capacity to tap emerging opportunities in India, Auto News, ET Auto     We have updated our terms and conditions and privacy policy Click \"Continue\" to accept and continue with ET AutoAccept the updated privacy &amp; cookie policyDear user, ET Auto privacy and cookie policy has been updated to align with the new data regulations in European Union. Please review and accept these changes below to continue using the website.You can see our privacy policy &amp; our cookie ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                  url  \\\n",
       "0                                              http://kokomoperspective.com/obituaries/jon-w-horton/article_b6ba8e1e-cb9c-11eb-9868-fb11b88b9778.html   \n",
       "1  https://auto.economictimes.indiatimes.com/news/auto-components/birla-precision-to-ramp-up-capacity-to-tap-emerging-opportunities-in-india/81254902   \n",
       "\n",
       "        date language  \\\n",
       "0 2021-06-13       en   \n",
       "1 2021-02-28       en   \n",
       "\n",
       "                                                                                                 title  \\\n",
       "0                                                   Jon W. Horton | Obituaries | kokomoperspective.com   \n",
       "1       Birla Precision to ramp up capacity to tap emerging opportunities in India, Auto News, ET Auto   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text  \n",
       "0  Jon W. Horton | Obituaries | kokomoperspective.comYou have permission to edit this article. EditCloseSign Up                        Log In                    Dashboard  LogoutMy Account Dashboard Profile Saved items LogoutCOVID-19Click here for the latest local news on COVID-19HomeAbout UsContact UsNewsLocalOpinionPoliticsNationalStateAgricultureLifestylesEngagements/Anniversaries/WeddingsAutosEntertainmentHealthHomesOutdoorsSportsNFLNCAAVitalsObituariesAutomotivee-EditionCouponsGalleries74¬∞...  \n",
       "1      Birla Precision to ramp up capacity to tap emerging opportunities in India, Auto News, ET Auto     We have updated our terms and conditions and privacy policy Click \"Continue\" to accept and continue with ET AutoAccept the updated privacy & cookie policyDear user, ET Auto privacy and cookie policy has been updated to align with the new data regulations in European Union. Please review and accept these changes below to continue using the website.You can see our privacy policy & our cookie ...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_path = 'https://storage.googleapis.com/msca-bdp-data-open/news/nlp_a_5_news.json'\n",
    "news_df = pd.read_json(news_path, orient='records', lines=True)\n",
    "\n",
    "print(f'Sample contains {news_df.shape[0]:,.0f} news articles')\n",
    "news_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Tweets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample contains 10,105 tweets\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1534565117614084096</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-06-08</td>\n",
       "      <td>Low Orbit Tourist üåçüì∑</td>\n",
       "      <td></td>\n",
       "      <td>Body &amp;amp; Assembly - Halewood - United Kingdom\\nüåç53.3504,-2.8352296,402m\\n\\nHalewood Body &amp;amp; Assembly is a Jaguar Land Rover factory in Halewood, England, and forms the major part of the Halewood complex which is shared with Ford who manufacture transmissions at the site. [Wikipedia] https://t.co/LPmCnZIaVt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1534565743429394439</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-06-08</td>\n",
       "      <td>CompleteCar.ie</td>\n",
       "      <td>RT</td>\n",
       "      <td>Land Rover Ireland has announced that the new Range Rover Sport starts at ‚Ç¨114,150, now on @completecar:\\n\\nhttps://t.co/TjGUkL3FYr https://t.co/QdVaEiJkjO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id lang       date                  name retweeted  \\\n",
       "0  1534565117614084096   en 2022-06-08  Low Orbit Tourist üåçüì∑             \n",
       "1  1534565743429394439   en 2022-06-08        CompleteCar.ie        RT   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                       text  \n",
       "0  Body &amp; Assembly - Halewood - United Kingdom\\nüåç53.3504,-2.8352296,402m\\n\\nHalewood Body &amp; Assembly is a Jaguar Land Rover factory in Halewood, England, and forms the major part of the Halewood complex which is shared with Ford who manufacture transmissions at the site. [Wikipedia] https://t.co/LPmCnZIaVt  \n",
       "1                                                                                                                                                               Land Rover Ireland has announced that the new Range Rover Sport starts at ‚Ç¨114,150, now on @completecar:\\n\\nhttps://t.co/TjGUkL3FYr https://t.co/QdVaEiJkjO  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_path = 'https://storage.googleapis.com/msca-bdp-data-open/tweets/nlp_a_5_tweets.json'\n",
    "tweets_df = pd.read_json(tweets_path, orient='records', lines=True)\n",
    "print(f'Sample contains {tweets_df.shape[0]:,.0f} tweets')\n",
    "tweets_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discard non-English results & Apply appropriate text cleaning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lmschwenke/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/lmschwenke/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "\n",
    "english_words = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
    "    # Remove hashtags (but keep the text after #)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    # Remove RT (retweet symbol)\n",
    "    text = re.sub(r'RT[\\s]+', '', text)\n",
    "    # Remove hyperlinks\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text)\n",
    "    # Remove newline characters\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    # Remove carriage return characters\n",
    "    text = re.sub(r'\\r', '', text)\n",
    "    # Remove \"&amp;\"\n",
    "    text = re.sub(r'&amp;', '', text)\n",
    "    # Remove other special characters and numbers\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    # Convert multiple spaces to a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Optionally, convert to lowercase\n",
    "    # text = text.lower()\n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    # Remove non-English words\n",
    "    text = ' '.join([word for word in text.split() if word.lower() in english_words])\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['tweets_clean'] = tweets_df['text'].parallel_apply(clean_text)\n",
    "news_df['text_clean'] = news_df['text'].parallel_apply(clean_text) \n",
    "news_df['title_clean'] = news_df['title'].parallel_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Body Assembly United Kingdom Body Assembly Jaguar Land Rover factory major part complex Ford manufacture site</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Land Rover new Range Rover Sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Land Rover Range Rover Top Speed With Ease On Autobahn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                    tweets_clean\n",
       "0  Body Assembly United Kingdom Body Assembly Jaguar Land Rover factory major part complex Ford manufacture site\n",
       "1                                                                               Land Rover new Range Rover Sport\n",
       "2                                                     New Land Rover Range Rover Top Speed With Ease On Autobahn"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[['tweets_clean']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W permission edit article Up Log In Dashboard Account Dashboard Profile Saved latest local news IN A stray shower thunderstorm possible Low F light A stray shower thunderstorm possible Low F light variable June Full latest local news COVID Support Local Journalism Now ever world needs trustworthy good journalism free Please support us making contribution Contribute W W June Health Ball Memorial He born August On married Donna The couple blessed two marriage worked Camp Quaker After worked St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision ramp capacity tap Auto News Auto We privacy policy Click Continue accept continue privacy user Auto privacy policy align new data Union Please review accept continue see privacy policy policy We use ensure best experience choose ignore message well assume happy receive track site origin track statistics consent state current serve content relevant identify Fingerprinting uniquely identify client Daily daily list important information industry read accepted Retail News Health News N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Global Hydrogen Station Current State Future Prognosis Passenger Hydrogen Fuel Cell HOME MAIL NEWS SPORTS FINANCE CELEBRITY STYLE WEATHER MOBILE Yahoo Sports Sign Mail Sign view mail Sports Home Sports Home Fantasy Fantasy Fantasy Football Fantasy Hockey Fantasy Basketball Fantasy Auto Auto Racing Fantasy Golf Fantasy Baseball Home Yahoo Sports Sports Junior Hockey Home Home More Football Home Soccer Soccer Soccer Home Premier League League A La World Cup Over Over Everything Yahoo Hockey Ho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            text_clean\n",
       "0  W permission edit article Up Log In Dashboard Account Dashboard Profile Saved latest local news IN A stray shower thunderstorm possible Low F light A stray shower thunderstorm possible Low F light variable June Full latest local news COVID Support Local Journalism Now ever world needs trustworthy good journalism free Please support us making contribution Contribute W W June Health Ball Memorial He born August On married Donna The couple blessed two marriage worked Camp Quaker After worked St...\n",
       "1  Precision ramp capacity tap Auto News Auto We privacy policy Click Continue accept continue privacy user Auto privacy policy align new data Union Please review accept continue see privacy policy policy We use ensure best experience choose ignore message well assume happy receive track site origin track statistics consent state current serve content relevant identify Fingerprinting uniquely identify client Daily daily list important information industry read accepted Retail News Health News N...\n",
       "2  Global Hydrogen Station Current State Future Prognosis Passenger Hydrogen Fuel Cell HOME MAIL NEWS SPORTS FINANCE CELEBRITY STYLE WEATHER MOBILE Yahoo Sports Sign Mail Sign view mail Sports Home Sports Home Fantasy Fantasy Fantasy Football Fantasy Hockey Fantasy Basketball Fantasy Auto Auto Racing Fantasy Golf Fantasy Baseball Home Yahoo Sports Sports Junior Hockey Home Home More Football Home Soccer Soccer Soccer Home Premier League League A La World Cup Over Over Everything Yahoo Hockey Ho..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df[['text_clean']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision ramp capacity tap Auto News Auto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Global Hydrogen Station Current State Future Prognosis Passenger Hydrogen Fuel Cell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           title_clean\n",
       "0                                                                                    W\n",
       "1                                           Precision ramp capacity tap Auto News Auto\n",
       "2  Global Hydrogen Station Current State Future Prognosis Passenger Hydrogen Fuel Cell"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df[['title_clean']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER) - Using NLTK for Organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a counter to keep track of organization frequencies\n",
    "organization_counter = Counter()\n",
    "\n",
    "# Define a function to extract organizations using NLTK\n",
    "def nltk_extract_organizations(text):\n",
    "    entities = []\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text)), binary=False):\n",
    "        if hasattr(chunk, 'label') and chunk.label() == 'ORGANIZATION':\n",
    "            entities.append(' '.join(c[0] for c in chunk))\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39725c06c8c3499b8141308e5ff115d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1444), Label(value='0 / 1444'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9edb27b4628c440cb228ccd2204074a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1431), Label(value='0 / 1431'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da159cad87c44e9489ced5461c28fda0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1431), Label(value='0 / 1431'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets_df['organizations'] = tweets_df['tweets_clean'].parallel_apply(nltk_extract_organizations)\n",
    "news_df['organizations_from_text'] = news_df['text_clean'].parallel_apply(nltk_extract_organizations)\n",
    "news_df['organizations_from_title'] = news_df['title_clean'].parallel_apply(nltk_extract_organizations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_body_dict = {\"Tweets\": tweets_df['organizations'],\n",
    "                  \"News Text\": news_df['organizations_from_text'],\n",
    "                  \"News Title\": news_df['organizations_from_title']}\n",
    "\n",
    "def get_top_orgs(text_body):\n",
    "    top_organizations_dict = {}\n",
    "\n",
    "    for key, text in text_body.items():\n",
    "        all_organizations = [org for org_list in text for org in org_list]\n",
    "        \n",
    "        # Update the organization frequencies\n",
    "        organization_counter.clear()\n",
    "        organization_counter.update(all_organizations)\n",
    "\n",
    "        # Find the most commonly mentioned organizations\n",
    "        most_common_organizations = organization_counter.most_common(20)\n",
    "        \n",
    "        top_organizations_dict[key] = most_common_organizations\n",
    "\n",
    "    return top_organizations_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Outputs: \n",
      "\n",
      "Tweets:\n",
      "Jaguar Land Rover: 575\n",
      "Land Rover: 553\n",
      "Jaguar Land Rover General: 265\n",
      "Land Rover Defender: 228\n",
      "Ford: 82\n",
      "SHAMELESS Health Board: 64\n",
      "Rover: 61\n",
      "Land Rover Range Rover: 53\n",
      "Jaguar Jeep Land Rover: 51\n",
      "Range Rover: 49\n",
      "Nestle Jaguar Land Rover: 47\n",
      "Jaguar: 43\n",
      "Grenadier Land Rover Defender Business Daily: 43\n",
      "Health Board: 41\n",
      "Jaguar Land: 38\n",
      "Jaguar Land Rover Driving Challenge: 37\n",
      "Subterranean Challenge: 36\n",
      "Honda: 29\n",
      "EU: 22\n",
      "Gravity Business Park: 20\n",
      "\n",
      "News Text:\n",
      "COVID: 9724\n",
      "Princess: 5707\n",
      "VERY: 5644\n",
      "LA: 4825\n",
      "US: 2932\n",
      "Duchess: 2501\n",
      "THE: 2498\n",
      "House: 2149\n",
      "NOT: 2127\n",
      "Mail Media: 1905\n",
      "ALL: 1433\n",
      "FIRST: 1280\n",
      "RELATED: 1277\n",
      "Lipa: 1228\n",
      "Republican: 1152\n",
      "MILLION: 1133\n",
      "City: 1131\n",
      "Vanity Fair: 1008\n",
      "Associated: 963\n",
      "Land Rover: 961\n",
      "\n",
      "News Title:\n",
      "Star News: 177\n",
      "Ford: 111\n",
      "Automotive News: 94\n",
      "Business Live: 59\n",
      "News: 44\n",
      "News Driven: 43\n",
      "Mail: 42\n",
      "COVID: 40\n",
      "Fast Lane Car: 33\n",
      "Land: 31\n",
      "Ford Escape: 31\n",
      "RAM: 27\n",
      "Auto News: 26\n",
      "Auto News Auto: 24\n",
      "Express Star: 22\n",
      "AWD: 21\n",
      "RAM Sale: 19\n",
      "Car Expert: 19\n",
      "AWD Sale: 16\n",
      "Senate: 15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_organizations = get_top_orgs(text_body_dict)\n",
    "\n",
    "print(\"NLTK Outputs: \\n\")\n",
    "for key, organizations in top_organizations.items():\n",
    "    print(f\"{key}:\")\n",
    "    for org, count in organizations:\n",
    "        print(f\"{org}: {count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Sentence Segmentation with NLTK for extracting Organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_extract_organizations_sentences(text):\n",
    "    entities = []\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenize each sentence into words and perform organization extraction\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(word_tokenize(sentence)), binary=False):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'ORGANIZATION':\n",
    "                entities.append(' '.join(c[0] for c in chunk))\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16938e9a1330421088f6e5d5df2e9edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1444), Label(value='0 / 1444'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683a411e646e4d469403443f6dde5074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1431), Label(value='0 / 1431'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83862f317704f2c90f68250750552b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1431), Label(value='0 / 1431'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets_df['organizations_sentences'] = tweets_df['tweets_clean'].parallel_apply(nltk_extract_organizations_sentences)\n",
    "news_df['organizations_from_text_sentences'] = news_df['text_clean'].parallel_apply(nltk_extract_organizations_sentences)\n",
    "news_df['organizations_from_title_sentences'] = news_df['title_clean'].parallel_apply(nltk_extract_organizations_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Outputs - Sentence Segmentation: \n",
      "\n",
      "Tweets:\n",
      "Land: 925\n",
      "Land Rover: 694\n",
      "LAND: 188\n",
      "ROVER: 128\n",
      "Duke Duchess: 96\n",
      "SHAMELESS: 93\n",
      "Land Rover Discovery: 87\n",
      "SHAMELESS Health Board Zimbabwe: 64\n",
      "UPDATE: 53\n",
      "Jaguar Land: 46\n",
      "Hospital: 36\n",
      "BaT Land Rover: 32\n",
      "FRANCHISE: 32\n",
      "Duke Duchess Jaguar Land Rover: 28\n",
      "Rover: 23\n",
      "BAE Hawk: 20\n",
      "Defender: 19\n",
      "NEW: 19\n",
      "Duke: 18\n",
      "Land Rover Which: 17\n",
      "\n",
      "News Text:\n",
      "COVID: 9724\n",
      "Princess: 5707\n",
      "VERY: 5644\n",
      "LA: 4825\n",
      "US: 2932\n",
      "Duchess: 2501\n",
      "THE: 2498\n",
      "House: 2149\n",
      "NOT: 2127\n",
      "Mail Media: 1905\n",
      "ALL: 1433\n",
      "FIRST: 1280\n",
      "RELATED: 1277\n",
      "Lipa: 1228\n",
      "Republican: 1152\n",
      "MILLION: 1133\n",
      "City: 1131\n",
      "Vanity Fair: 1008\n",
      "Associated: 963\n",
      "Land Rover: 961\n",
      "\n",
      "News Title:\n",
      "Star News: 177\n",
      "Ford: 111\n",
      "Automotive News: 94\n",
      "Business Live: 59\n",
      "News: 44\n",
      "News Driven: 43\n",
      "Mail: 42\n",
      "COVID: 40\n",
      "Fast Lane Car: 33\n",
      "Land: 31\n",
      "Ford Escape: 31\n",
      "RAM: 27\n",
      "Auto News: 26\n",
      "Auto News Auto: 24\n",
      "Express Star: 22\n",
      "AWD: 21\n",
      "RAM Sale: 19\n",
      "Car Expert: 19\n",
      "AWD Sale: 16\n",
      "Senate: 15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_body_dict = {\"Tweets\": tweets_df['organizations_sentences'],\n",
    "                  \"News Text\": news_df['organizations_from_text_sentences'],\n",
    "                  \"News Title\": news_df['organizations_from_title_sentences']}\n",
    "\n",
    "top_organizations = get_top_orgs(text_body_dict)\n",
    "\n",
    "print(\"NLTK Outputs - Sentence Segmentation: \\n\")\n",
    "for key, organizations in top_organizations.items():\n",
    "    print(f\"{key}:\")\n",
    "    for org, count in organizations:\n",
    "        print(f\"{org}: {count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER) - Using SpaCy for Organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.2\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "from spacy import displacy\n",
    "spacy.prefer_gpu()\n",
    "# spacy.require_gpu()\n",
    "\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SpaCy Models:  \n",
    "- en_core_web_sm: English multi-task CNN trained on OntoNotes. Size ‚Äì 11 MB\n",
    "- en_core_web_md: English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Size ‚Äì 91 MB\n",
    "- en_core_web_lg: English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Size ‚Äì 789 MB\n",
    "- en_core_web_trf: English transformer pipeline (roberta-base). Components: transformer, tagger, parser, ner, attribute_ruler, lemmatizer.  Size - 438 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not spacy.util.is_package(\"en_core_web_lg\"):\n",
    "    # If not, download and install it\n",
    "    spacy.cli.download(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SpaCy model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp = spacy.load(\"en_core_web_md\")\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "# nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking active pipeline components\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_extract_organizations(text):\n",
    "    doc = nlp(text)\n",
    "    organizations = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
    "    return organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce2e5fed6ee4b0d9be5efe0a168c18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1444), Label(value='0 / 1444'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0dbddb664f1408c956b092938fde441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1431), Label(value='0 / 1431'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1631088ffe34addbbee96da9ae0680f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1431), Label(value='0 / 1431'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the extract_organizations function to the \"tweets_clean\" column\n",
    "tweets_df['organizations_spacy'] = tweets_df['tweets_clean'].parallel_apply(spacy_extract_organizations)\n",
    "news_df['organizations_from_text_spacy'] = news_df['text_clean'].parallel_apply(spacy_extract_organizations)\n",
    "news_df['organizations_from_title_spacy'] = news_df['title_clean'].parallel_apply(spacy_extract_organizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_body_dict = {\"Tweets\": tweets_df['organizations_spacy'],\n",
    "                  \"News Text\": news_df['organizations_from_text_spacy'],\n",
    "                  \"News Title\": news_df['organizations_from_title_spacy']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy Outputs: \n",
      "\n",
      "Tweets:\n",
      "Jaguar Land Rover: 575\n",
      "Land Rover: 553\n",
      "Jaguar Land Rover General: 265\n",
      "Land Rover Defender: 228\n",
      "Ford: 82\n",
      "SHAMELESS Health Board: 64\n",
      "Rover: 61\n",
      "Land Rover Range Rover: 53\n",
      "Jaguar Jeep Land Rover: 51\n",
      "Range Rover: 49\n",
      "Nestle Jaguar Land Rover: 47\n",
      "Jaguar: 43\n",
      "Grenadier Land Rover Defender Business Daily: 43\n",
      "Health Board: 41\n",
      "Jaguar Land: 38\n",
      "Jaguar Land Rover Driving Challenge: 37\n",
      "Subterranean Challenge: 36\n",
      "Honda: 29\n",
      "EU: 22\n",
      "Gravity Business Park: 20\n",
      "\n",
      "News Text:\n",
      "Ford: 4895\n",
      "House: 3352\n",
      "Honda: 2509\n",
      "White House: 2445\n",
      "Vanity Fair: 2034\n",
      "Mail Media: 1905\n",
      "Duchess: 1874\n",
      "Apple: 1481\n",
      "Palace: 1480\n",
      "United: 1479\n",
      "Vogue: 1476\n",
      "Jeep: 1076\n",
      "Royal Family: 1044\n",
      "Shop: 1041\n",
      "Land Rover: 972\n",
      "Dodge: 946\n",
      "Range Rover: 920\n",
      "Genesis: 842\n",
      "Royal: 806\n",
      "Jaguar: 796\n",
      "\n",
      "News Title:\n",
      "Daily Mail: 1484\n",
      "Ford: 264\n",
      "Star News: 156\n",
      "Honda: 127\n",
      "Autocar: 113\n",
      "Automotive News: 97\n",
      "Express Star: 84\n",
      "Car Dealer Magazine: 77\n",
      "Daily Times News: 70\n",
      "Jeep: 46\n",
      "Dodge: 45\n",
      "Auto News Auto: 41\n",
      "Daily Record: 36\n",
      "The China Post: 35\n",
      "Mirror: 33\n",
      "Jaguar: 27\n",
      "Star: 27\n",
      "Jaguar Land Rover: 25\n",
      "EU: 20\n",
      "Times: 18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_organizations = get_top_orgs(text_body_dict)\n",
    "\n",
    "print(\"SpaCy Outputs: \\n\")\n",
    "for key, organizations in top_organizations.items():\n",
    "    print(f\"{key}:\")\n",
    "    for org, count in organizations:\n",
    "        print(f\"{org}: {count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Sentence Segmentation with SpaCy for extracting Organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_extract_organizations_sentences(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    organizations = []\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        sentence_organizations = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
    "        organizations.extend(sentence_organizations)\n",
    "\n",
    "    return organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a2dc6004d2421a90b05d627b31b954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1444), Label(value='0 / 1444'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "645516181609466d9279e597ad6598eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1431), Label(value='0 / 1431'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e591ef8110427ba961286e24398c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1431), Label(value='0 / 1431'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the extract_organizations function to the \"tweets_clean\" column\n",
    "tweets_df['organizations_spacy_sentences'] = tweets_df['tweets_clean'].parallel_apply(spacy_extract_organizations_sentences)\n",
    "news_df['organizations_from_text_spacy_sentences'] = news_df['text_clean'].parallel_apply(spacy_extract_organizations_sentences)\n",
    "news_df['organizations_from_title_spacy_sentences'] = news_df['title_clean'].parallel_apply(spacy_extract_organizations_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_body_dict = {\"Tweets\": tweets_df['organizations_spacy_sentences'],\n",
    "                  \"News Text\": news_df['organizations_from_text_spacy_sentences'],\n",
    "                  \"News Title\": news_df['organizations_from_title_spacy_sentences']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy Outputs - Sentences: \n",
      "\n",
      "Tweets:\n",
      "Jaguar Land Rover: 575\n",
      "Land Rover: 553\n",
      "Jaguar Land Rover General: 265\n",
      "Land Rover Defender: 228\n",
      "Ford: 82\n",
      "SHAMELESS Health Board: 64\n",
      "Rover: 61\n",
      "Land Rover Range Rover: 53\n",
      "Jaguar Jeep Land Rover: 51\n",
      "Range Rover: 49\n",
      "Nestle Jaguar Land Rover: 47\n",
      "Jaguar: 43\n",
      "Grenadier Land Rover Defender Business Daily: 43\n",
      "Health Board: 41\n",
      "Jaguar Land: 38\n",
      "Jaguar Land Rover Driving Challenge: 37\n",
      "Subterranean Challenge: 36\n",
      "Honda: 29\n",
      "EU: 22\n",
      "Gravity Business Park: 20\n",
      "\n",
      "News Text:\n",
      "Ford: 4895\n",
      "House: 3352\n",
      "Honda: 2509\n",
      "White House: 2445\n",
      "Vanity Fair: 2034\n",
      "Mail Media: 1905\n",
      "Duchess: 1874\n",
      "Apple: 1481\n",
      "Palace: 1480\n",
      "United: 1479\n",
      "Vogue: 1476\n",
      "Jeep: 1076\n",
      "Royal Family: 1044\n",
      "Shop: 1041\n",
      "Land Rover: 972\n",
      "Dodge: 946\n",
      "Range Rover: 920\n",
      "Genesis: 842\n",
      "Royal: 806\n",
      "Jaguar: 796\n",
      "\n",
      "News Title:\n",
      "Daily Mail: 1484\n",
      "Ford: 264\n",
      "Star News: 156\n",
      "Honda: 127\n",
      "Autocar: 113\n",
      "Automotive News: 97\n",
      "Express Star: 84\n",
      "Car Dealer Magazine: 77\n",
      "Daily Times News: 70\n",
      "Jeep: 46\n",
      "Dodge: 45\n",
      "Auto News Auto: 41\n",
      "Daily Record: 36\n",
      "The China Post: 35\n",
      "Mirror: 33\n",
      "Jaguar: 27\n",
      "Star: 27\n",
      "Jaguar Land Rover: 25\n",
      "EU: 20\n",
      "Times: 18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_organizations = get_top_orgs(text_body_dict)\n",
    "\n",
    "print(\"SpaCy Outputs - Sentences: \\n\")\n",
    "for key, organizations in top_organizations.items():\n",
    "    print(f\"{key}:\")\n",
    "    for org, count in organizations:\n",
    "        print(f\"{org}: {count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER) - Using SpaCy for Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_extract_locations(text):\n",
    "    doc = nlp(text)\n",
    "    organizations = [ent.text for ent in doc.ents if ent.label_ == \"GPE\"]\n",
    "    return organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993d34aa908b42e19bb342c13b76f4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1444), Label(value='0 / 1444'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec714f5e134d433882c52ffe05c01f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1431), Label(value='0 / 1431'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4975d76b7224ce69abd70131a7962de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1431), Label(value='0 / 1431'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets_df['locations_spacy'] = tweets_df['tweets_clean'].parallel_apply(spacy_extract_locations)\n",
    "news_df['locations_from_text_spacy'] = news_df['text_clean'].parallel_apply(spacy_extract_locations)\n",
    "news_df['locations_from_title_spacy'] = news_df['title_clean'].parallel_apply(spacy_extract_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_body_dict = {\"Tweets\": tweets_df['locations_spacy'],\n",
    "                  \"News Text\": news_df['locations_from_text_spacy'],\n",
    "                  \"News Title\": news_df['locations_from_title_spacy']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy Outputs: \n",
      "\n",
      "Tweets:\n",
      "Russia: 464\n",
      "Zimbabwe: 87\n",
      "BaT: 42\n",
      "Brunswick: 40\n",
      "US: 36\n",
      "China: 29\n",
      "Somerset: 28\n",
      "Mungofa: 20\n",
      "Cayman: 16\n",
      "North West: 15\n",
      "Jordan: 13\n",
      "Park: 11\n",
      "st: 9\n",
      "Japan: 8\n",
      "LA: 5\n",
      "Derby: 4\n",
      "Tableau: 3\n",
      "Canada: 3\n",
      "Iceland: 3\n",
      "Arusha: 2\n",
      "\n",
      "News Text:\n",
      "LA: 14597\n",
      "US: 12184\n",
      "New York City: 6926\n",
      "New York: 4093\n",
      "China: 2422\n",
      "Las: 1572\n",
      "Russia: 1504\n",
      "Canada: 960\n",
      "Trump: 824\n",
      "Turkey: 787\n",
      "Japan: 778\n",
      "San: 635\n",
      "Brazil: 623\n",
      "Michigan: 571\n",
      "TOWN: 533\n",
      "Jordan: 514\n",
      "Boston: 487\n",
      "New Jersey: 417\n",
      "Colorado: 380\n",
      "Orange County: 374\n",
      "\n",
      "News Title:\n",
      "US: 132\n",
      "North York: 72\n",
      "China: 33\n",
      "Russia: 30\n",
      "Scotia: 29\n",
      "Saskatoon: 26\n",
      "Midland: 18\n",
      "Tilbury: 18\n",
      "Japan: 13\n",
      "Somerset: 11\n",
      "Commonwealth: 11\n",
      "New York: 11\n",
      "LA: 9\n",
      "Canada: 9\n",
      "Cobourg: 8\n",
      "Michigan: 7\n",
      "Guinea: 6\n",
      "Colorado: 6\n",
      "Brazil: 5\n",
      "Turkey: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_locations = get_top_orgs(text_body_dict)\n",
    "\n",
    "print(\"SpaCy Outputs: \\n\")\n",
    "for key, locations in top_locations.items():\n",
    "    print(f\"{key}:\")\n",
    "    for loc, count in locations:\n",
    "        print(f\"{loc}: {count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Sentence Segmentation with SpaCy for extracting Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_extract_locations_sentences(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    organizations = []\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        sentence_organizations = [ent.text for ent in doc.ents if ent.label_ == \"GPE\"]\n",
    "        organizations.extend(sentence_organizations)\n",
    "\n",
    "    return organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa88b55741240a9a6ab89505a0ffa4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1444), Label(value='0 / 1444'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcaffb15848e4a36a7227972a4abb32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1431), Label(value='0 / 1431'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55c2125b6cf4f0ba554cfe569255282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1431), Label(value='0 / 1431'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the extract_organizations function to the \"tweets_clean\" column\n",
    "tweets_df['locations_spacy_sentences'] = tweets_df['tweets_clean'].parallel_apply(spacy_extract_locations_sentences)\n",
    "news_df['locations_from_text_spacy_sentences'] = news_df['text_clean'].parallel_apply(spacy_extract_locations_sentences)\n",
    "news_df['locations_from_title_spacy_sentences'] = news_df['title_clean'].parallel_apply(spacy_extract_locations_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_body_dict = {\"Tweets\": tweets_df['locations_spacy_sentences'],\n",
    "                  \"News Text\": news_df['locations_from_text_spacy_sentences'],\n",
    "                  \"News Title\": news_df['locations_from_title_spacy_sentences']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy Outputs - Sentences: \n",
      "\n",
      "Tweets:\n",
      "Russia: 464\n",
      "Zimbabwe: 87\n",
      "BaT: 42\n",
      "Brunswick: 40\n",
      "US: 36\n",
      "China: 29\n",
      "Somerset: 28\n",
      "Mungofa: 20\n",
      "Cayman: 16\n",
      "North West: 15\n",
      "Jordan: 13\n",
      "Park: 11\n",
      "st: 9\n",
      "Japan: 8\n",
      "LA: 5\n",
      "Derby: 4\n",
      "Tableau: 3\n",
      "Canada: 3\n",
      "Iceland: 3\n",
      "Arusha: 2\n",
      "\n",
      "News Text:\n",
      "LA: 14597\n",
      "US: 12184\n",
      "New York City: 6926\n",
      "New York: 4093\n",
      "China: 2422\n",
      "Las: 1572\n",
      "Russia: 1504\n",
      "Canada: 960\n",
      "Trump: 824\n",
      "Turkey: 787\n",
      "Japan: 778\n",
      "San: 635\n",
      "Brazil: 623\n",
      "Michigan: 571\n",
      "TOWN: 533\n",
      "Jordan: 514\n",
      "Boston: 487\n",
      "New Jersey: 417\n",
      "Colorado: 380\n",
      "Orange County: 374\n",
      "\n",
      "News Title:\n",
      "US: 132\n",
      "North York: 72\n",
      "China: 33\n",
      "Russia: 30\n",
      "Scotia: 29\n",
      "Saskatoon: 26\n",
      "Midland: 18\n",
      "Tilbury: 18\n",
      "Japan: 13\n",
      "Somerset: 11\n",
      "Commonwealth: 11\n",
      "New York: 11\n",
      "LA: 9\n",
      "Canada: 9\n",
      "Cobourg: 8\n",
      "Michigan: 7\n",
      "Guinea: 6\n",
      "Colorado: 6\n",
      "Brazil: 5\n",
      "Turkey: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_locations = get_top_orgs(text_body_dict)\n",
    "\n",
    "print(\"SpaCy Outputs - Sentences: \\n\")\n",
    "for key, locations in top_locations.items():\n",
    "    print(f\"{key}:\")\n",
    "    for loc, count in locations:\n",
    "        print(f\"{loc}: {count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER) - Using NLTK for Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a counter to keep track of organization frequencies\n",
    "organization_counter = Counter()\n",
    "\n",
    "# Define a function to extract organizations using NLTK\n",
    "def nltk_extract_locations(text):\n",
    "    entities = []\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text)), binary=False):\n",
    "        if hasattr(chunk, 'label') and chunk.label() == 'GPE':\n",
    "            entities.append(' '.join(c[0] for c in chunk))\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6026a492be04a17bd3e4ec0e2676848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1444), Label(value='0 / 1444'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3d46ac883f4fd9b79a3f820f5293da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1431), Label(value='0 / 1431'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644e47d3bcfa4f6cbe7dcf3fe062085b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1431), Label(value='0 / 1431'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets_df['locations_nltk'] = tweets_df['tweets_clean'].parallel_apply(nltk_extract_locations)\n",
    "news_df['locations_from_text_nltk'] = news_df['text_clean'].parallel_apply(nltk_extract_locations)\n",
    "news_df['locations_from_title_nltk'] = news_df['title_clean'].parallel_apply(nltk_extract_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_body_dict = {\"Tweets\": tweets_df['locations_nltk'],\n",
    "                  \"News Text\": news_df['locations_from_text_nltk'],\n",
    "                  \"News Title\": news_df['locations_from_title_nltk']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Outputs: \n",
      "\n",
      "Tweets:\n",
      "Land: 1851\n",
      "Russia: 454\n",
      "New: 121\n",
      "LAND: 117\n",
      "Prince: 86\n",
      "Car: 64\n",
      "Meet: 41\n",
      "New Land: 29\n",
      "Great: 28\n",
      "China: 25\n",
      "South: 19\n",
      "EU: 19\n",
      "Latest: 17\n",
      "Hi: 16\n",
      "Check: 16\n",
      "Mission: 16\n",
      "Good: 16\n",
      "Whilst: 15\n",
      "Please: 13\n",
      "Boss: 13\n",
      "\n",
      "News Text:\n",
      "New York City: 6932\n",
      "New York: 4612\n",
      "New: 4580\n",
      "Prince: 4387\n",
      "China: 2525\n",
      "Palace: 2492\n",
      "Amelia: 2071\n",
      "South: 1736\n",
      "United: 1720\n",
      "Crown: 1519\n",
      "North: 1385\n",
      "West: 1289\n",
      "Swift: 1238\n",
      "Russia: 1203\n",
      "San: 1097\n",
      "Moss: 1052\n",
      "Land: 920\n",
      "Jordan: 842\n",
      "Grand: 771\n",
      "US: 709\n",
      "\n",
      "News Title:\n",
      "New: 143\n",
      "Prince: 140\n",
      "China: 92\n",
      "Land: 82\n",
      "Russia: 30\n",
      "North York: 28\n",
      "Jaguar: 25\n",
      "Latest: 25\n",
      "South: 24\n",
      "US: 21\n",
      "Covid: 21\n",
      "EU: 20\n",
      "Grenadier: 18\n",
      "Electric: 18\n",
      "Car: 17\n",
      "Japan: 15\n",
      "German: 15\n",
      "Enjoy: 14\n",
      "Queen: 14\n",
      "Best: 13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_locations = get_top_orgs(text_body_dict)\n",
    "\n",
    "print(\"NLTK Outputs: \\n\")\n",
    "for key, locations in top_locations.items():\n",
    "    print(f\"{key}:\")\n",
    "    for loc, count in locations:\n",
    "        print(f\"{loc}: {count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Sentence Segmentation with NLTK for extracting Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_extract_locations_sentences(text):\n",
    "    entities = []\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenize each sentence into words and perform organization extraction\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(word_tokenize(sentence)), binary=False):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'GPE':\n",
    "                entities.append(' '.join(c[0] for c in chunk))\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46124317dac84080acd79904cf4ba3ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1444), Label(value='0 / 1444'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70129d33b78140089631a193558ca63c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1431), Label(value='0 / 1431'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d159595051e54a1b9bdc7ec634af8eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1431), Label(value='0 / 1431'))), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tweets_df['organizations_nltk_sentences'] = tweets_df['tweets_clean'].parallel_apply(nltk_extract_locations_sentences)\n",
    "news_df['organizations_from_text_nltk_sentences'] = news_df['text_clean'].parallel_apply(nltk_extract_locations_sentences)\n",
    "news_df['organizations_from_title_nltk_sentences'] = news_df['title_clean'].parallel_apply(nltk_extract_locations_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_body_dict = {\"Tweets\": tweets_df['organizations_nltk_sentences'],\n",
    "                  \"News Text\": news_df['organizations_from_text_nltk_sentences'],\n",
    "                  \"News Title\": news_df['organizations_from_title_nltk_sentences']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Outputs: \n",
      "\n",
      "Tweets:\n",
      "Land: 1851\n",
      "Russia: 454\n",
      "New: 121\n",
      "LAND: 117\n",
      "Prince: 86\n",
      "Car: 64\n",
      "Meet: 41\n",
      "New Land: 29\n",
      "Great: 28\n",
      "China: 25\n",
      "South: 19\n",
      "EU: 19\n",
      "Latest: 17\n",
      "Hi: 16\n",
      "Check: 16\n",
      "Mission: 16\n",
      "Good: 16\n",
      "Whilst: 15\n",
      "Please: 13\n",
      "Boss: 13\n",
      "\n",
      "News Text:\n",
      "New York City: 6932\n",
      "New York: 4612\n",
      "New: 4580\n",
      "Prince: 4387\n",
      "China: 2525\n",
      "Palace: 2492\n",
      "Amelia: 2071\n",
      "South: 1736\n",
      "United: 1720\n",
      "Crown: 1519\n",
      "North: 1385\n",
      "West: 1289\n",
      "Swift: 1238\n",
      "Russia: 1203\n",
      "San: 1097\n",
      "Moss: 1052\n",
      "Land: 920\n",
      "Jordan: 842\n",
      "Grand: 771\n",
      "US: 709\n",
      "\n",
      "News Title:\n",
      "New: 143\n",
      "Prince: 140\n",
      "China: 92\n",
      "Land: 82\n",
      "Russia: 30\n",
      "North York: 28\n",
      "Jaguar: 25\n",
      "Latest: 25\n",
      "South: 24\n",
      "US: 21\n",
      "Covid: 21\n",
      "EU: 20\n",
      "Grenadier: 18\n",
      "Electric: 18\n",
      "Car: 17\n",
      "Japan: 15\n",
      "German: 15\n",
      "Enjoy: 14\n",
      "Queen: 14\n",
      "Best: 13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_locations = get_top_orgs(text_body_dict)\n",
    "\n",
    "print(\"NLTK Outputs: \\n\")\n",
    "for key, locations in top_locations.items():\n",
    "    print(f\"{key}:\")\n",
    "    for loc, count in locations:\n",
    "        print(f\"{loc}: {count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions:\n",
    "\n",
    "## *Part 1: Main Organization/Company:* \n",
    "\n",
    "### After extractinging entities separately from Tweets, Article Titles, and Article Texts, the top organization mentioned in the Tweets data is **Jaguar Land Rover** and top organization mentioned in the Articles is **Ford**. Both sentence and non-sentence segmentation across the NLTK and SpaCy packages returned similar results.\n",
    "\n",
    "## *Part 2: Other Organizations/Companies:* \n",
    "\n",
    "### Other companies recognized are Ford, Honda, Appled, Daily Mail, Jeep, and more\n",
    "\n",
    "## *Part 3: Location of Events:* \n",
    "\n",
    "### After extractinging entities separately from Tweets, Article Titles, and Article Texts, the most frequently mentioned locations (countries, states, cities, regions, etc.) are Russia, China, New York City, New York, the United States (US), LA, Turkey, Japan, Zimbabwe, Brunswick, and more.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9739a3ee864b5f74d703657e4ddbfdf7f7ccbe6642a458f796596a18bcc5b2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
